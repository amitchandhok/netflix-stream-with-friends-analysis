# ğŸ“º Simulating a Data Science Project at Netflix: "Stream with Friends"

## ğŸ” Project Overview

This project simulates the real-world workflow of a **Data Scientist** working at Netflix, tasked with evaluating a new feature: **â€œStream with Friends.â€** The feature allows paid users to co-watch Netflix content in sync with friends remotely. The goal of this project is to demonstrate how a data scientist would drive the feature from **idea to launch**, with a focus on experimentation, SQL + Python analysis, churn prediction, and clear business recommendations.

---

## ğŸ§  Why This Project?

In real data science roles, success doesnâ€™t come from just writing code â€” it comes from solving business problems through **hypothesis-driven analysis**, **cross-functional collaboration**, and **decision-making under ambiguity**.

This project replicates the full lifecycle of a feature rollout:
- From defining metrics and data collection requirements
- To experimentation and modeling
- To generating dashboards and executive recommendations

It goes far beyond a toy dataset or competition â€” it's meant to reflect how real product data scientists think and work.

---

## ğŸ› ï¸ Tools Used

- **SQL** (PostgreSQL)
- **Python** (Pandas, Scikit-learn, Matplotlib, Seaborn)
- **A/B Testing** logic and simulation
- **Logistic Regression** for churn prediction
- **Data schema simulation** and tracking plans
- **Dashboard** (streamlit)

---

## ğŸ§­ Project Structure

### **1. Pre-Implementation Phase: Planning for Success**

#### ğŸ§© Business Context
Before any code is written, itâ€™s essential to define:
- **What does â€œsuccessâ€ mean?**
  - Higher retention?
  - More engagement?
  - Reduced churn?

#### ğŸ“ˆ Success Metrics
- **Primary metric**: Increase in Day 30 Retention
- **Secondary metrics**: 
  - Feature adoption rate
  - Invite-to-accept conversion
  - Watch session length
  - Number of co-watched sessions

#### ğŸ—ƒï¸ Event Tracking & Schema Design
As a data scientist, I would work with engineering to ensure we track the right behaviors:
- Events: `invite_sent`, `invite_accepted`, `co_watch_start`, `co_watch_end`, `chat_opened`
- Tables: `users`, `sessions`, `invites`, `events`, `feature_exposure`

ğŸ“ _Why?_ Without proper tracking, post-launch analytics would be biased, limited, or completely impossible. Good data begins with intentional logging.

---

### **2. Experimentation Phase: A/B Test Design & Analysis**

#### ğŸ¯ Hypothesis
> Users who have access to â€œStream with Friendsâ€ will show higher retention and engagement than those who do not.

#### ğŸ§ª Test Design
- **Randomized Controlled Trial** with 50/50 split
- User-level randomization
- **Power analysis** to determine minimum sample size
- Success threshold: 95% statistical significance on primary metric

#### ğŸ” A/B Test Analysis
Using simulated SQL and Python data:
- Compare engagement metrics between groups
- Use statistical testing (t-test or non-parametric)
- Segment results by age group, region, device type

ğŸ“ _Why?_ A/B testing lets us make causal claims about the impact of the feature, not just correlations.

---

### **3. Churn Prediction Modeling**

#### ğŸ¤– Modeling Goal
Use exposure to the feature, engagement, and user behavior to predict likelihood of churn within 30 days.

#### ğŸ”¢ Features used:
- `feature_exposed` (binary)
- `co_watch_count`
- `avg_watch_time`
- `invite_accepted_rate`
- `account_age_days`
- `region`

Model: **Logistic Regression**  
Output: **Churn probability score**

ğŸ“ _Why?_ Understanding which users are likely to churn helps Netflix target retention campaigns and feature tweaks for at-risk users.

---

### **4. Dashboarding & Reporting**

A high-level dashboard mockup showing:
- Feature adoption rate
- Retention delta (test vs control)
- Active users over time
- Segmentation breakdown (age, region)

ğŸ“ _Why?_ Stakeholders need to understand the impact in real time â€” dashboards make data accessible and actionable.

---

### **5. Final Recommendations**

Based on all findings:
- Recommend full rollout or further iteration
- Suggest which user groups to prioritize
- Propose follow-up experiments (e.g., chat feature, rewards for co-watching)

ğŸ“ _Why?_ Data scientists need to close the loop â€” helping business partners act on insights, not just observe them.

---

## ğŸ§ª Sample Deliverables

- âœ… SQL queries to analyze feature adoption and retention
- âœ… Python scripts to clean data, run tests, and model churn
- âœ… Visuals comparing A/B test outcomes
- âœ… Summary memo with rollout recommendation

---

## ğŸ—‚ï¸ Project Directory

## ğŸš€ Future Enhancements

- Simulate multi-feature testing (e.g., add chat)
- Implement retention cohort analysis
- Incorporate time-series forecasting for usage
- Use a real BI dashboard tool (Streamlit / Power BI)

---
